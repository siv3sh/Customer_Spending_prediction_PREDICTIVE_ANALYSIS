{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Annual Spending Score Prediction\n",
        "\n",
        "## E-commerce Customer Behavior Analysis and Predictive Modeling\n",
        "\n",
        "**Objective:** Predict customer annual spending score to optimize targeted marketing campaigns\n",
        "\n",
        "**Dataset:** Mall Customers - Contains customer demographic details, purchase history, and income information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Set style for better visualizations\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except:\n",
        "        plt.style.use('ggplot')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Explore Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('Mall_Customers.csv')\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset information\n",
        "print(\"Dataset Info:\")\n",
        "df.info()\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\\nDataset Statistics:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing Values:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "print(\"\\nMissing Percentage:\")\n",
        "print((missing_values / len(df)) * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Check unique values in categorical columns\n",
        "print(\"\\nUnique values in Gender:\")\n",
        "print(df['Gender'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of Spending Score (Target Variable)\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(df['Spending Score (1-100)'], bins=20, edgecolor='black', color='skyblue')\n",
        "plt.title('Distribution of Spending Score', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Spending Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.boxplot(df['Spending Score (1-100)'])\n",
        "plt.title('Box Plot of Spending Score', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Spending Score')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.violinplot(y=df['Spending Score (1-100)'])\n",
        "plt.title('Violin Plot of Spending Score', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean Spending Score: {df['Spending Score (1-100)'].mean():.2f}\")\n",
        "print(f\"Median Spending Score: {df['Spending Score (1-100)'].median():.2f}\")\n",
        "print(f\"Std Spending Score: {df['Spending Score (1-100)'].std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation Analysis\n",
        "# Create correlation matrix\n",
        "numeric_cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "            square=True, linewidths=2, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix - Features vs Spending Score', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelation with Spending Score:\")\n",
        "print(correlation_matrix['Spending Score (1-100)'].sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relationship between Age and Spending Score\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(df['Age'], df['Spending Score (1-100)'], alpha=0.6, color='coral')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Spending Score')\n",
        "plt.title('Age vs Spending Score', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.regplot(x='Age', y='Spending Score (1-100)', data=df, scatter_kws={'alpha':0.6})\n",
        "plt.title('Age vs Spending Score (with regression line)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "age_bins = pd.cut(df['Age'], bins=5)\n",
        "df['Age_Group'] = age_bins\n",
        "sns.boxplot(x='Age_Group', y='Spending Score (1-100)', data=df)\n",
        "plt.title('Spending Score by Age Groups', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relationship between Annual Income and Spending Score\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(df['Annual Income (k$)'], df['Spending Score (1-100)'], alpha=0.6, color='green')\n",
        "plt.xlabel('Annual Income (k$)')\n",
        "plt.ylabel('Spending Score')\n",
        "plt.title('Annual Income vs Spending Score', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.regplot(x='Annual Income (k$)', y='Spending Score (1-100)', data=df, scatter_kws={'alpha':0.6})\n",
        "plt.title('Annual Income vs Spending Score (with regression line)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "income_bins = pd.cut(df['Annual Income (k$)'], bins=5)\n",
        "df['Income_Group'] = income_bins\n",
        "sns.boxplot(x='Income_Group', y='Spending Score (1-100)', data=df)\n",
        "plt.title('Spending Score by Income Groups', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gender Analysis\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "gender_counts = df['Gender'].value_counts()\n",
        "plt.pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Gender Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.boxplot(x='Gender', y='Spending Score (1-100)', data=df)\n",
        "plt.title('Spending Score by Gender', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.violinplot(x='Gender', y='Spending Score (1-100)', data=df)\n",
        "plt.title('Spending Score Distribution by Gender', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSpending Score Statistics by Gender:\")\n",
        "print(df.groupby('Gender')['Spending Score (1-100)'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3D Relationship Analysis\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Color by spending score\n",
        "scatter = ax.scatter(df['Age'], df['Annual Income (k$)'], df['Spending Score (1-100)'], \n",
        "                    c=df['Spending Score (1-100)'], cmap='viridis', alpha=0.6, s=50)\n",
        "\n",
        "ax.set_xlabel('Age', fontsize=12)\n",
        "ax.set_ylabel('Annual Income (k$)', fontsize=12)\n",
        "ax.set_zlabel('Spending Score', fontsize=12)\n",
        "ax.set_title('3D Relationship: Age, Income, and Spending Score', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.colorbar(scatter, label='Spending Score')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for feature engineering\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Handle missing values (if any)\n",
        "print(\"Missing values before handling:\")\n",
        "print(df_processed.isnull().sum())\n",
        "\n",
        "# Fill any missing values (though dataset appears complete)\n",
        "df_processed = df_processed.ffill().bfill()\n",
        "\n",
        "print(\"\\nMissing values after handling:\")\n",
        "print(df_processed.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Derive new features\n",
        "\n",
        "# 1. Age groups\n",
        "df_processed['Age_Group_Category'] = pd.cut(df_processed['Age'], \n",
        "                                             bins=[0, 30, 40, 50, 100], \n",
        "                                             labels=['Young', 'Middle', 'Senior', 'Elderly'])\n",
        "\n",
        "# 2. Income groups\n",
        "df_processed['Income_Group_Category'] = pd.cut(df_processed['Annual Income (k$)'], \n",
        "                                                bins=[0, 40, 70, 100, 150], \n",
        "                                                labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "# 3. Income to Age ratio (spending power indicator)\n",
        "df_processed['Income_Age_Ratio'] = df_processed['Annual Income (k$)'] / (df_processed['Age'] + 1)\n",
        "\n",
        "# 4. Age squared (non-linear relationship)\n",
        "df_processed['Age_Squared'] = df_processed['Age'] ** 2\n",
        "\n",
        "# 5. Income squared (non-linear relationship)\n",
        "df_processed['Income_Squared'] = df_processed['Annual Income (k$)'] ** 2\n",
        "\n",
        "# 6. Interaction feature: Age * Income\n",
        "df_processed['Age_Income_Interaction'] = df_processed['Age'] * df_processed['Annual Income (k$)']\n",
        "\n",
        "# 7. Spending capacity (normalized)\n",
        "df_processed['Spending_Capacity'] = (df_processed['Annual Income (k$)'] - df_processed['Annual Income (k$)'].min()) / \\\n",
        "                                     (df_processed['Annual Income (k$)'].max() - df_processed['Annual Income (k$)'].min())\n",
        "\n",
        "# 8. Is Young and High Income\n",
        "df_processed['Young_High_Income'] = ((df_processed['Age'] < 35) & (df_processed['Annual Income (k$)'] > 70)).astype(int)\n",
        "\n",
        "# 9. Is Senior and Low Income\n",
        "df_processed['Senior_Low_Income'] = ((df_processed['Age'] > 50) & (df_processed['Annual Income (k$)'] < 50)).astype(int)\n",
        "\n",
        "print(\"New features created:\")\n",
        "print(df_processed.columns.tolist())\n",
        "print(\"\\nDataset shape:\", df_processed.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample of engineered features\n",
        "print(\"Sample of engineered features:\")\n",
        "feature_cols = ['Age', 'Annual Income (k$)', 'Age_Group_Category', 'Income_Group_Category', \n",
        "                'Income_Age_Ratio', 'Age_Squared', 'Income_Squared', 'Age_Income_Interaction',\n",
        "                'Spending_Capacity', 'Young_High_Income', 'Senior_Low_Income']\n",
        "df_processed[feature_cols].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X = df_processed.drop(['CustomerID', 'Spending Score (1-100)', 'Age_Group', 'Income_Group'], axis=1, errors='ignore')\n",
        "y = df_processed['Spending Score (1-100)']\n",
        "\n",
        "print(\"Features shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)\n",
        "print(\"\\nFeature columns:\")\n",
        "print(X.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "\n",
        "for col in X.select_dtypes(include=['object', 'category']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "    print(f\"Encoded {col}\")\n",
        "\n",
        "print(\"\\nAll features are now numeric:\")\n",
        "print(X.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"Features scaled successfully!\")\n",
        "print(\"\\nScaled training data statistics:\")\n",
        "print(X_train_scaled.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Building - Simple Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Model 1: Linear Regression\n",
        "print(\"=\"*60)\n",
        "print(\"SIMPLE MODEL: Linear Regression\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "simple_model = LinearRegression()\n",
        "simple_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_simple = simple_model.predict(X_train_scaled)\n",
        "y_test_pred_simple = simple_model.predict(X_test_scaled)\n",
        "\n",
        "# Metrics\n",
        "train_rmse_simple = np.sqrt(mean_squared_error(y_train, y_train_pred_simple))\n",
        "test_rmse_simple = np.sqrt(mean_squared_error(y_test, y_test_pred_simple))\n",
        "train_mae_simple = mean_absolute_error(y_train, y_train_pred_simple)\n",
        "test_mae_simple = mean_absolute_error(y_test, y_test_pred_simple)\n",
        "train_r2_simple = r2_score(y_train, y_train_pred_simple)\n",
        "test_r2_simple = r2_score(y_test, y_test_pred_simple)\n",
        "\n",
        "print(f\"\\nTraining Metrics:\")\n",
        "print(f\"  RMSE: {train_rmse_simple:.4f}\")\n",
        "print(f\"  MAE:  {train_mae_simple:.4f}\")\n",
        "print(f\"  R²:   {train_r2_simple:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Metrics:\")\n",
        "print(f\"  RMSE: {test_rmse_simple:.4f}\")\n",
        "print(f\"  MAE:  {test_mae_simple:.4f}\")\n",
        "print(f\"  R²:   {test_r2_simple:.4f}\")\n",
        "\n",
        "# Feature importance (coefficients)\n",
        "feature_importance_simple = pd.DataFrame({\n",
        "    'Feature': X_train_scaled.columns,\n",
        "    'Coefficient': simple_model.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features (by absolute coefficient):\")\n",
        "print(feature_importance_simple.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Building - Complex Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complex Model 1: Random Forest with Hyperparameter Tuning\n",
        "print(\"=\"*60)\n",
        "print(\"COMPLEX MODEL 1: Random Forest Regressor\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Hyperparameter tuning for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='neg_mean_squared_error', \n",
        "                              n_jobs=-1, verbose=1)\n",
        "\n",
        "print(\"\\nPerforming Grid Search for Random Forest...\")\n",
        "rf_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters: {rf_grid_search.best_params_}\")\n",
        "print(f\"Best CV score (neg MSE): {rf_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Use best model\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_rf = best_rf_model.predict(X_train_scaled)\n",
        "y_test_pred_rf = best_rf_model.predict(X_test_scaled)\n",
        "\n",
        "# Metrics\n",
        "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))\n",
        "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
        "train_mae_rf = mean_absolute_error(y_train, y_train_pred_rf)\n",
        "test_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
        "train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
        "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
        "\n",
        "print(f\"\\nTraining Metrics:\")\n",
        "print(f\"  RMSE: {train_rmse_rf:.4f}\")\n",
        "print(f\"  MAE:  {train_mae_rf:.4f}\")\n",
        "print(f\"  R²:   {train_r2_rf:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Metrics:\")\n",
        "print(f\"  RMSE: {test_rmse_rf:.4f}\")\n",
        "print(f\"  MAE:  {test_mae_rf:.4f}\")\n",
        "print(f\"  R²:   {test_r2_rf:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complex Model 2: Gradient Boosting with Hyperparameter Tuning\n",
        "print(\"=\"*60)\n",
        "print(\"COMPLEX MODEL 2: Gradient Boosting Regressor\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Hyperparameter tuning for Gradient Boosting\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "gb_model = GradientBoostingRegressor(random_state=42)\n",
        "gb_grid_search = GridSearchCV(gb_model, gb_param_grid, cv=5, scoring='neg_mean_squared_error', \n",
        "                              n_jobs=-1, verbose=1)\n",
        "\n",
        "print(\"\\nPerforming Grid Search for Gradient Boosting...\")\n",
        "gb_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters: {gb_grid_search.best_params_}\")\n",
        "print(f\"Best CV score (neg MSE): {gb_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Use best model\n",
        "best_gb_model = gb_grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_gb = best_gb_model.predict(X_train_scaled)\n",
        "y_test_pred_gb = best_gb_model.predict(X_test_scaled)\n",
        "\n",
        "# Metrics\n",
        "train_rmse_gb = np.sqrt(mean_squared_error(y_train, y_train_pred_gb))\n",
        "test_rmse_gb = np.sqrt(mean_squared_error(y_test, y_test_pred_gb))\n",
        "train_mae_gb = mean_absolute_error(y_train, y_train_pred_gb)\n",
        "test_mae_gb = mean_absolute_error(y_test, y_test_pred_gb)\n",
        "train_r2_gb = r2_score(y_train, y_train_pred_gb)\n",
        "test_r2_gb = r2_score(y_test, y_test_pred_gb)\n",
        "\n",
        "print(f\"\\nTraining Metrics:\")\n",
        "print(f\"  RMSE: {train_rmse_gb:.4f}\")\n",
        "print(f\"  MAE:  {train_mae_gb:.4f}\")\n",
        "print(f\"  R²:   {train_r2_gb:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Metrics:\")\n",
        "print(f\"  RMSE: {test_rmse_gb:.4f}\")\n",
        "print(f\"  MAE:  {test_mae_gb:.4f}\")\n",
        "print(f\"  R²:   {test_r2_gb:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Comparison and Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "models_comparison = pd.DataFrame({\n",
        "    'Model': ['Linear Regression (Simple)', 'Random Forest (Complex)', 'Gradient Boosting (Complex)'],\n",
        "    'Train RMSE': [train_rmse_simple, train_rmse_rf, train_rmse_gb],\n",
        "    'Test RMSE': [test_rmse_simple, test_rmse_rf, test_rmse_gb],\n",
        "    'Train MAE': [train_mae_simple, train_mae_rf, train_mae_gb],\n",
        "    'Test MAE': [test_mae_simple, test_mae_rf, test_mae_gb],\n",
        "    'Train R²': [train_r2_simple, train_r2_rf, train_r2_gb],\n",
        "    'Test R²': [test_r2_simple, test_r2_rf, test_r2_gb]\n",
        "})\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(models_comparison.to_string(index=False))\n",
        "\n",
        "# Determine best model based on test R² (primary) and test RMSE (secondary)\n",
        "best_model_idx = models_comparison['Test R²'].idxmax()\n",
        "best_model_name = models_comparison.loc[best_model_idx, 'Model']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"BEST MODEL: {best_model_name}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Test R² Score: {models_comparison.loc[best_model_idx, 'Test R²']:.4f}\")\n",
        "print(f\"Test RMSE: {models_comparison.loc[best_model_idx, 'Test RMSE']:.4f}\")\n",
        "print(f\"Test MAE: {models_comparison.loc[best_model_idx, 'Test MAE']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# RMSE Comparison\n",
        "axes[0].bar(models_comparison['Model'], models_comparison['Test RMSE'], color=['skyblue', 'lightgreen', 'coral'])\n",
        "axes[0].set_title('Test RMSE Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('RMSE')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# MAE Comparison\n",
        "axes[1].bar(models_comparison['Model'], models_comparison['Test MAE'], color=['skyblue', 'lightgreen', 'coral'])\n",
        "axes[1].set_title('Test MAE Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('MAE')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# R² Comparison\n",
        "axes[2].bar(models_comparison['Model'], models_comparison['Test R²'], color=['skyblue', 'lightgreen', 'coral'])\n",
        "axes[2].set_title('Test R² Comparison', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel('R² Score')\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model predictions for visualization\n",
        "if best_model_name == 'Linear Regression (Simple)':\n",
        "    best_predictions = y_test_pred_simple\n",
        "    best_model_obj = simple_model\n",
        "elif best_model_name == 'Random Forest (Complex)':\n",
        "    best_predictions = y_test_pred_rf\n",
        "    best_model_obj = best_rf_model\n",
        "else:\n",
        "    best_predictions = y_test_pred_gb\n",
        "    best_model_obj = best_gb_model\n",
        "\n",
        "# Actual vs Predicted\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Actual vs Predicted Scatter\n",
        "axes[0, 0].scatter(y_test, best_predictions, alpha=0.6, color='steelblue')\n",
        "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
        "axes[0, 0].set_xlabel('Actual Spending Score', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Predicted Spending Score', fontsize=12)\n",
        "axes[0, 0].set_title(f'Actual vs Predicted - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Residuals\n",
        "residuals = y_test - best_predictions\n",
        "axes[0, 1].scatter(best_predictions, residuals, alpha=0.6, color='coral')\n",
        "axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[0, 1].set_xlabel('Predicted Spending Score', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Residuals', fontsize=12)\n",
        "axes[0, 1].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Distribution of Residuals\n",
        "axes[1, 0].hist(residuals, bins=20, edgecolor='black', color='lightgreen', alpha=0.7)\n",
        "axes[1, 0].axvline(x=0, color='r', linestyle='--', lw=2)\n",
        "axes[1, 0].set_xlabel('Residuals', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1, 0].set_title('Distribution of Residuals', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Actual vs Predicted Line Plot\n",
        "test_indices = range(len(y_test))\n",
        "axes[1, 1].plot(test_indices, y_test.values, 'o-', label='Actual', alpha=0.7, color='steelblue')\n",
        "axes[1, 1].plot(test_indices, best_predictions, 's-', label='Predicted', alpha=0.7, color='coral')\n",
        "axes[1, 1].set_xlabel('Test Sample Index', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Spending Score', fontsize=12)\n",
        "axes[1, 1].set_title('Actual vs Predicted Over Test Samples', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance Visualization (for tree-based models)\n",
        "if hasattr(best_model_obj, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X_train_scaled.columns,\n",
        "        'Importance': best_model_obj.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(data=feature_importance.head(15), x='Importance', y='Feature', palette='viridis')\n",
        "    plt.title(f'Top 15 Feature Importance - {best_model_name}', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nTop 15 Most Important Features:\")\n",
        "    print(feature_importance.head(15).to_string(index=False))\n",
        "else:\n",
        "    # For Linear Regression, show coefficients\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X_train_scaled.columns,\n",
        "        'Coefficient': best_model_obj.coef_\n",
        "    }).sort_values('Coefficient', key=abs, ascending=False)\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(data=feature_importance.head(15), x='Coefficient', y='Feature', palette='coolwarm')\n",
        "    plt.title(f'Top 15 Feature Coefficients - {best_model_name}', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Coefficient Value', fontsize=12)\n",
        "    plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nTop 15 Most Important Features (by absolute coefficient):\")\n",
        "    print(feature_importance.head(15).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Interpretation and Reporting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"INTERPRETATION AND INSIGHTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. MODEL PERFORMANCE SUMMARY:\")\n",
        "print(f\"   Best Model: {best_model_name}\")\n",
        "print(f\"   Test R² Score: {models_comparison.loc[best_model_idx, 'Test R²']:.4f}\")\n",
        "print(f\"   Test RMSE: {models_comparison.loc[best_model_idx, 'Test RMSE']:.4f}\")\n",
        "print(f\"   Test MAE: {models_comparison.loc[best_model_idx, 'Test MAE']:.4f}\")\n",
        "\n",
        "r2_score_val = models_comparison.loc[best_model_idx, 'Test R²']\n",
        "if r2_score_val > 0.7:\n",
        "    performance_level = \"Excellent\"\n",
        "elif r2_score_val > 0.5:\n",
        "    performance_level = \"Good\"\n",
        "elif r2_score_val > 0.3:\n",
        "    performance_level = \"Moderate\"\n",
        "else:\n",
        "    performance_level = \"Poor\"\n",
        "\n",
        "print(f\"   Performance Level: {performance_level}\")\n",
        "print(f\"   The model explains {r2_score_val*100:.2f}% of the variance in spending scores.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n2. CUSTOMER ATTRIBUTES INFLUENCING ANNUAL SPENDING:\")\n",
        "\n",
        "# Get feature importance for interpretation\n",
        "if hasattr(best_model_obj, 'feature_importances_'):\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': X_train_scaled.columns,\n",
        "        'Importance': best_model_obj.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "else:\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': X_train_scaled.columns,\n",
        "        'Importance': np.abs(best_model_obj.coef_)\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\n   Top 10 Most Influential Attributes:\")\n",
        "for idx, row in importance_df.head(10).iterrows():\n",
        "    print(f\"   {idx+1}. {row['Feature']}: {row['Importance']:.4f}\")\n",
        "\n",
        "# Analyze original features\n",
        "original_features = ['Age', 'Annual Income (k$)', 'Gender']\n",
        "print(\"\\n   Key Insights on Original Features:\")\n",
        "\n",
        "# Age impact\n",
        "if 'Age' in importance_df['Feature'].values:\n",
        "    age_importance = importance_df[importance_df['Feature'] == 'Age']['Importance'].values[0]\n",
        "    print(f\"   - Age has importance: {age_importance:.4f}\")\n",
        "\n",
        "# Income impact\n",
        "if 'Annual Income (k$)' in importance_df['Feature'].values:\n",
        "    income_importance = importance_df[importance_df['Feature'] == 'Annual Income (k$)']['Importance'].values[0]\n",
        "    print(f\"   - Annual Income has importance: {income_importance:.4f}\")\n",
        "\n",
        "# Gender impact\n",
        "if 'Gender' in importance_df['Feature'].values:\n",
        "    gender_importance = importance_df[importance_df['Feature'] == 'Gender']['Importance'].values[0]\n",
        "    print(f\"   - Gender has importance: {gender_importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n3. INSIGHTS FOR IMPROVING TARGETED MARKETING:\")\n",
        "\n",
        "# Analyze customer segments\n",
        "df_processed['Predicted_Spending'] = best_model_obj.predict(scaler.transform(X))\n",
        "df_processed['Spending_Category'] = pd.cut(df_processed['Predicted_Spending'], \n",
        "                                           bins=[0, 30, 50, 70, 100], \n",
        "                                           labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "print(\"\\n   Customer Segments by Predicted Spending:\")\n",
        "segment_analysis = df_processed.groupby('Spending_Category').agg({\n",
        "    'Age': 'mean',\n",
        "    'Annual Income (k$)': 'mean',\n",
        "    'Gender': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'N/A'\n",
        "}).round(2)\n",
        "\n",
        "print(segment_analysis)\n",
        "\n",
        "print(\"\\n   Marketing Recommendations:\")\n",
        "print(\"   1. High Spending Customers:\")\n",
        "high_spenders = df_processed[df_processed['Spending_Category'].isin(['High', 'Very High'])]\n",
        "if len(high_spenders) > 0:\n",
        "    print(f\"      - Average Age: {high_spenders['Age'].mean():.1f} years\")\n",
        "    print(f\"      - Average Income: ${high_spenders['Annual Income (k$)'].mean():.1f}k\")\n",
        "    print(f\"      - Focus: Premium products, loyalty programs, exclusive offers\")\n",
        "\n",
        "print(\"\\n   2. Low Spending Customers:\")\n",
        "low_spenders = df_processed[df_processed['Spending_Category'] == 'Low']\n",
        "if len(low_spenders) > 0:\n",
        "    print(f\"      - Average Age: {low_spenders['Age'].mean():.1f} years\")\n",
        "    print(f\"      - Average Income: ${low_spenders['Annual Income (k$)'].mean():.1f}k\")\n",
        "    print(f\"      - Focus: Discount campaigns, budget-friendly options, value propositions\")\n",
        "\n",
        "print(\"\\n   3. Demographic Targeting:\")\n",
        "gender_spending = df_processed.groupby('Gender')['Predicted_Spending'].mean()\n",
        "print(f\"      - Average predicted spending by gender:\")\n",
        "for gender, spending in gender_spending.items():\n",
        "    print(f\"        {gender}: {spending:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n4. MODEL PERFORMANCE EXPLANATION:\")\n",
        "print(f\"   The {best_model_name} achieved the following performance:\")\n",
        "print(f\"   - R² Score of {models_comparison.loc[best_model_idx, 'Test R²']:.4f} indicates that the model\")\n",
        "print(f\"     explains {models_comparison.loc[best_model_idx, 'Test R²']*100:.2f}% of the variance in spending scores.\")\n",
        "print(f\"   - RMSE of {models_comparison.loc[best_model_idx, 'Test RMSE']:.4f} means on average, predictions\")\n",
        "print(f\"     deviate from actual values by approximately {models_comparison.loc[best_model_idx, 'Test RMSE']:.2f} points.\")\n",
        "print(f\"   - MAE of {models_comparison.loc[best_model_idx, 'Test MAE']:.4f} represents the average absolute\")\n",
        "print(f\"     error in spending score predictions.\")\n",
        "\n",
        "print(\"\\n5. SUGGESTED IMPROVEMENTS:\")\n",
        "print(\"   1. Collect More Data: Increase dataset size for better generalization\")\n",
        "print(\"   2. Additional Features: Include purchase history, browsing patterns, product preferences\")\n",
        "print(\"   3. Feature Engineering: Create more domain-specific features (e.g., customer lifetime value)\")\n",
        "print(\"   4. Ensemble Methods: Combine multiple models for better predictions\")\n",
        "print(\"   5. Regular Updates: Retrain model periodically with new data\")\n",
        "print(\"   6. Cross-Validation: Use k-fold cross-validation for more robust evaluation\")\n",
        "print(\"   7. Hyperparameter Optimization: Use more sophisticated methods (Bayesian Optimization)\")\n",
        "print(\"   8. Feature Selection: Remove less important features to reduce overfitting\")\n",
        "\n",
        "print(\"\\n6. REAL-WORLD APPLICATIONS:\")\n",
        "print(\"   1. Personalized Marketing: Target customers with high predicted spending scores\")\n",
        "print(\"   2. Budget Allocation: Allocate marketing budget based on customer segments\")\n",
        "print(\"   3. Product Recommendations: Suggest products to customers based on spending patterns\")\n",
        "print(\"   4. Customer Retention: Identify high-value customers for retention programs\")\n",
        "print(\"   5. Pricing Strategy: Adjust pricing for different customer segments\")\n",
        "print(\"   6. Inventory Management: Stock products preferred by high-spending customers\")\n",
        "print(\"   7. Campaign Optimization: Design campaigns targeting specific spending score ranges\")\n",
        "print(\"   8. Customer Acquisition: Identify characteristics of high-spending customers for targeting\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Additional Analysis - Customer Segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create customer segments based on predicted spending\n",
        "df_processed['Customer_Segment'] = pd.cut(df_processed['Predicted_Spending'], \n",
        "                                           bins=[0, 30, 50, 70, 100], \n",
        "                                           labels=['Low Spender', 'Medium Spender', 'High Spender', 'VIP'])\n",
        "\n",
        "# Visualize segments\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Segment distribution\n",
        "segment_counts = df_processed['Customer_Segment'].value_counts()\n",
        "axes[0, 0].pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 0].set_title('Customer Segment Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Age distribution by segment\n",
        "sns.boxplot(data=df_processed, x='Customer_Segment', y='Age', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Age Distribution by Customer Segment', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Income distribution by segment\n",
        "sns.boxplot(data=df_processed, x='Customer_Segment', y='Annual Income (k$)', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Income Distribution by Customer Segment', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gender distribution by segment\n",
        "segment_gender = pd.crosstab(df_processed['Customer_Segment'], df_processed['Gender'])\n",
        "segment_gender.plot(kind='bar', ax=axes[1, 1], color=['skyblue', 'lightcoral'])\n",
        "axes[1, 1].set_title('Gender Distribution by Customer Segment', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Customer Segment')\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "axes[1, 1].legend(title='Gender')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Customer Segment Summary:\")\n",
        "print(segment_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Final Summary and Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create final summary dataframe\n",
        "final_summary = pd.DataFrame({\n",
        "    'CustomerID': df_processed['CustomerID'],\n",
        "    'Age': df_processed['Age'],\n",
        "    'Gender': df_processed['Gender'],\n",
        "    'Annual_Income_k': df_processed['Annual Income (k$)'],\n",
        "    'Actual_Spending_Score': df_processed['Spending Score (1-100)'],\n",
        "    'Predicted_Spending_Score': df_processed['Predicted_Spending'],\n",
        "    'Customer_Segment': df_processed['Customer_Segment'],\n",
        "    'Prediction_Error': abs(df_processed['Spending Score (1-100)'] - df_processed['Predicted_Spending'])\n",
        "})\n",
        "\n",
        "print(\"Final Summary Dataset:\")\n",
        "print(final_summary.head(10))\n",
        "\n",
        "# Save to CSV\n",
        "final_summary.to_csv('customer_spending_predictions.csv', index=False)\n",
        "print(\"\\nPredictions saved to 'customer_spending_predictions.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nKey Takeaways:\")\n",
        "print(f\"1. Best Model: {best_model_name}\")\n",
        "print(f\"2. Model Performance: R² = {models_comparison.loc[best_model_idx, 'Test R²']:.4f}\")\n",
        "print(f\"3. The model can predict customer spending scores with reasonable accuracy\")\n",
        "print(f\"4. Customer attributes like Age, Income, and derived features significantly influence spending\")\n",
        "print(f\"5. The model can be used for targeted marketing campaigns and customer segmentation\")\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
